---
title: "Topic Modeling"
author: "Suheng Yao"
date: "2024-11-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(topicmodels)
library(tidytext)
library(dplyr)
library(lexicon)
library(ggplot2)
library(tidyr)
library(ldatuning)
library(topicmodels)
library(tm)
library(factoextra)
library(wordcloud)
```

```{r}
df <- read.csv("movie_plots.csv")
```

```{r}
# Find document word counts
df_word <- df %>%
  unnest_tokens(word, Plot)

word_counts <- df_word %>%
  anti_join(stop_words, by="word") %>%
  count(Movie.Name, word, sort = TRUE)

# Use lexicon to remove common first names
data("freq_first_names")
given_name <- tolower(freq_first_names$Name)
word_counts <- word_counts %>%
  filter(!(word %in% given_name))

print(word_counts)
```
As the table above shown, the most used word in all those plots is "stile" with 17 counts in the movie King of Pecos.

```{r, warning=FALSE}
# LDA on films

# First create a DocumentTermMatrix for further topic modeling
film_dtm <- word_counts %>%
  cast_dtm(Movie.Name, word, n)

# Evaluate the number of topics (k) from 4 to 10
result <- FindTopicsNumber(
  film_dtm,
  topics = seq(4, 10, by = 1),
  metrics = c("CaoJuan2009", "Arun2010", "Griffiths2004", "Deveaud2014"),
  method = "Gibbs",            
  control = list(seed = 724),
  mc.cores = 2,                
  verbose = TRUE
)

FindTopicsNumber_plot(result)


# Use LDA to create a 10 topic model
film_lda <- LDA(film_dtm, k = 6, control = list(seed = 724))
film_topics <- tidy(film_lda, matrix="gamma")
head(film_topics)
```
From the plot created above, the best number of topic could be 6, which means that k=6.
```{r}
top_movie <- film_topics %>%
  group_by(topic) %>%
  slice_max(gamma) %>%
  ungroup()

print(top_movie)
```
As shown in the table above, we can find out the most related movie in each topic. After that, I will try to name each topic and see the content within each topic:
```{r}
# All movies in topic 1
topic1_word <- film_topics %>%
  filter(topic == 1) %>%
  arrange(-gamma)
head(topic1_word)
```

```{r}
# All movies in topic 2
topic2_word <- film_topics %>%
  filter(topic == 2) %>%
  arrange(-gamma)
head(topic2_word)
```

```{r}
# All movies in topic 3
topic3_word <- film_topics %>%
  filter(topic == 3) %>%
  arrange(-gamma)
head(topic3_word)
```

```{r}
# All movies in topic 4
topic4_word <- film_topics %>%
  filter(topic == 4) %>%
  arrange(-gamma)
head(topic4_word)
```
```{r}
# All movies in topic 5
topic5_word <- film_topics %>%
  filter(topic == 5) %>%
  arrange(-gamma)
head(topic5_word)
```
```{r}
topic6_word <- film_topics %>%
  filter(topic == 6) %>%
  arrange(-gamma)
head(topic6_word)
```
After finding out what is in each of those topics, we can use k-means clustering to cluster those data together:
```{r}
# Read in the movie plots with genres data
df1 <- read.csv("movie_plots_with_genres.csv")
num_genre <- df1 %>%
  group_by(Genre) %>%
  summarise(count = n()) %>%
  arrange(desc(count))
print(num_genre)
```
As shown in the table here, there are 8 genres, and the genre with most number of movies is "western".

```{r}
wider_data <- film_topics %>%
  pivot_wider(
    names_from = "topic",
    values_from = "gamma"
  )

wider_data <- wider_data %>% drop_na()
```

```{r}
# Use k-means to create the clusters
# First determine the optimal number of k
fviz_nbclust(wider_data %>% select(-document), kmeans, method = "wss")

```
From the scree plot above, the elbow point is at 6, so the optimized number of cluster k should be 6.

```{r}
cluster <- kmeans(wider_data %>% select(-document), 6)
fviz_cluster(cluster, data = wider_data %>% select(-document))
```
The graph may not be very clear, we can find out what actually in each cluster:
```{r, warning=FALSE, message=FALSE}
wider_data$cluster <- cluster$cluster

cluster1 <- wider_data %>%
  filter(cluster == 1) %>%
  select(c(document, cluster))

df <- rename(df, document=Movie.Name)

cluster1 <- left_join(cluster1, df, by="document")
cluster1 <- cluster1 %>%
  unnest_tokens(word, Plot)

# Set seed for consistent results
set.seed(123)

cluster1 %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(
    words = word,
    freq = n,
    max.words = 100,
    scale = c(3, 0.5),       
    min.freq = 2,            
    colors = brewer.pal(8, "Dark2"),  
    random.order = FALSE,    
    rot.per = 0.2
  ))

```

